---
title: "Data paper"
output:
  github_document: default
  pdf_document:
    toc: yes
  html_notebook:
    highlight: zenburn
    theme: spacelab
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    highlight: zenburn
    theme: spacelab
    toc: yes
    toc_float: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE, cache=TRUE}
setwd("~/Documents/0PhD/0Cleveland/Processing/Data_paper/")
```

### F. Ehtemam^1+^, H. Wang^2+^ and A. van den Bogert^2^  
^1^ University of Maryland & ^2^ Cleveland State University  

^+^ F. Ehtemam and H. Wang have contributed equally to this work.  

&ensp; Copyright &copy; 2017 Cleveland State University  

***

The most important question before discussing anything is that if publishing a data paper will prevent us from publishing future analysis based on the data or not. This page lists publishers that do not have any problem with the data being published earlier:  
https://f1000research.com/data-policies  
It is quite a large list so I think we can have the dataset published. 

## General considerations  

### Article name
Suggestions for the name of the article:  
1. "Kinematics and kinetics of normal and perturbed walking on the treadmill: a comprehensive gait dataset" or variants of this  
2.  
3.  

### Where to publish
#### Peer-reviewed (data journals)
Some major data journals:  

* Scientific Data  
    + Publisher: Nature  
    + Fee: $1350
    + Data hosting: Up to 100 GB free at figshare
    + Link: https://www.nature.com/sdata/publish/submission-guidelines
    
* GigaScience Data Notes
    + Publisher: Oxford Academic
    + Fee: $1015
    + Data hosting: Up to 1 TB free at their GigaDB repository
    + Link: https://academic.oup.com/gigascience/pages/data_note
* Data in Brief
    + Publisher: Elsevier
    + Fee: $500
    + Data hosting: Not available
    + Link: https://www.elsevier.com/journals/data-in-brief/2352-3409/guide-for-authors  

#### Peer-reviewed (general)

There are journals that are not specifically targeted towards publishing data papers but they will accept data papers as well (e.g. PeerJ). However, these journals are open access too and they charge similar publication fees as data journals mentioned above.

#### Non peer-reviewed
Alternatively one can publish their article on many repositories available online and host the data in a separate repository. The advantage is that this will be free but the authors don't get any feedback on their methods and data practices.  
Some of the famous online repositories for publication:

 * https://arxiv.org/
 * https://www.biorxiv.org/
 * https://osf.io/preprints/
 * https://peerj.com/preprints-search/
 * https://f1000research.com/

### Funding options

UMD funds 50% of the cost for open access publishing (https://www.lib.umd.edu/oa/openaccessfund). I don't know if a similar thing is available at CSU or not. Most universities these days have funds to support open access publishing. It could be through their libraries or a separate office. It doesn't hurt to ask if CSU has this.

## Statement of significance

An important part of any publication is to show its contribution to what is already available. Without a significant contribution there is no publication. The publication of this dataset has three major and six minor (arguably) contributions:

Major:

* Availability of EMG data
* Availability of accelerations
* Availability of processed data

Minor:

* Providing the data in the format readable by OpenSim
* Providing the 2D joint moments
* The data provides responses to two different amplitudes of perturbation.
* The data provides responses to two different types of mechanical perturbations (AP vs. ML).
* It provides data for more subjects than previous datasets.
* It provides more data per subject per condition compared to any other dataset (we have 16 minutes of walking for each amplitude of perturbation while PeerJ data had 8 minutes).


## Data organization

### Raw data  <a name="Raw data"></a> 

It is extremely important to publish the raw data. Any processed data has followed a certain method of processing that can make future applications limited or impossible due to the fact that different research questions require different methods of processing. So researchers must have access to the raw data. However, raw data does not necessarily mean unstructured data. Even the raw data has to be structured in a way that makes it easy for the user to understand and access different types of data within the dataset. So I think the following raw data have to be written in separate files and labeled appropriately:

* Kinematics
* GRFs
* EMGs
* Accelerations
* Treadmill files (speeds and accelerations)
* Designed perturbations

In addition to this I suggest an organization that separates normal walking and antero-posterior and medio-lateral perturbations. Furthermore, the data can be uploaded in two sets: a set called 21 subjects and a set called 15 subjects. While the 15 subjects were the same as the ones in 21 subjects set, these 15 are the ones for which EMGs and accelerations are available. This separation makes it easier for people with certain research questions to access the data. 

### Processed data

As important as publishing the raw data is, so is providing some sort of processed data. Biomechanics is an interdisciplinary field with people from very different backgrounds. Not everyone is tech-savvy enough to program scripts that allows them to analyze the raw data. What type of processed data one publishes depends on the field of study.  
It is common for studies in biomechanics to look at time normalized gait data. In addition to research questions that use this type of data in their investigations, this can be a highly useful educational resource. I was always disappointed by the lack of a simple dataset from couple of subjects that I could use to practice the knowledge I got from biomechanics courses through exploring different analysis methods using the data. We are going to change this by publishing the following time normalized data:

* Marker data
* Joint angles
* GRFs
* EMG
* Accelerations
* 2D joint moments (sagittal plane)

We will also include the graphs showing averages of processed data across all cycles to make it easy for people to compare it to what they already know from literature. In addition, we will provide the scripts we used to process the data. This includes a reference to Jason's GaitAnalysisToolkit we used for gap filling kinematics and the codes from Sandy and Huawei on inertial compensation.

## Addressing heterogeneities in the data structure

Rich datasets are rarely homogeneous in their structures. Changes in the experiment design, technical issues during data collection or other factors can result in heterogeneity. As an example, the PeerJ dataset has some subjects for which GRFs were collected using wooden supports under the treadmill that turned out to greatly affect the quality of the GRF data and made GRFs practically useless for those few subjects. Other examples from the PeerJ dataset and other datasets exist. Heterogeneous structure is not necessarily a limitation or a flaw of the dataset. For instance, in our data collection we forgot to put the supports under the treadmill for subject 23. While this may look like a flaw at the first glance, it resulted in us having a subject for which the unloaded trial is collected for every trial (I can explain later why having this could be beneficial). But despite some applications that may benefit from heterogeneities, a heterogeneous structure is mostly inconvenient during processing. Because of this, heterogeneity in structure has to be addressed in any data publication. Here are the items that contribute to a heterogeneous structure in our dataset with 24 subjects:

* Data collection could not be completed with reasonable number of trials finished for subjects 5 and 9. We have so far excluded them from analysis.
* EMG and acceleration data were collected for the last 15 subjects only.
* For subjects 13 and 14, the medio-lateral (ML) perturbation trials were not collected due to the time constraint.
* For subjects 2 and 3, the treadmill markers were not used. This means GRFs cannot be compensated.
* For subject 4, the last ML trial (speed of 1.6 m/s) was not collected because of treadmill noise.
* For subject 24, the last two unloaded trials of ML perturbation were not collected because of treadmill noise.
* For subject 23, treadmill supports were not used during AP trials and as a result unloaded data was collected for all 13 trials.
* For subject 12, the measurement of treadmill speed for trial 8 does not exist. 

I think the first issue need not be discussed in the manuscript. We can present only 21 subjects. The second and third issues I think should be clearly mentioned in the manuscript. The rest of the issues I think could be mentioned in the codebook accompanying the uploaded dataset (I think they are all already documented in metadata files). We can discuss if they have to be highlighted in the manuscript or not.

## Agenda

Here is a list of steps (some of them already done) in order to prepare the raw and processed datasets before the manuscript is written:

* Structure the raw data according to the suggestion from above (see [Raw data](#Raw data))
* Fill all the gaps in kinematics using Jason's package (done in the spring)
* Do the belt speed compensation and pitch sway compensation on GRFs (done by Huawei)
* Calculate 2D joint moments using leg2d code. (done by Huawei)
* Have all steps of processing replicated at Maryland and Cleveland to make sure we get the same processed data. (in progress)
* Time normalize the data and write it to appropriate formats (both CSV and OpenSim formats)
* Prepare graphs of averaged values across all cycles
* Write a codebook that explains the data structure and the file formats. This codebook has to be uploaded with the data and metadata files. (Has been done but the current version has to be expanded with some more details)













                          
  







